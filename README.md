<h2> Knowledge Distillation Papers </h2>



<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(1).pdf" style="text-decoration:none;">Do Deep Nets Really Need to be Deep?</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(2).pdf" style="text-decoration:none;">
FitNets: Hints for Thin Deep Nets</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(3).pdf" style="text-decoration:none;">Distilling the Knowledge in a Neural Network</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(4).pdf" style="text-decoration:none;">Recurrent neural network training with dark knowledge transfer</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(5).pdf" style="text-decoration:none;">Unifying distillation and privileged information</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(6).pdf" style="text-decoration:none;">Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(7).pdf" style="text-decoration:none;">Net2Net: Accelerating Learning via Knowledge Transfer</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(8).pdf" style="text-decoration:none;"> Adapting Models to Signal Degradation using Distillation </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(9).pdf" style="text-decoration:none;">Sequence-Level Knowledge Distillation</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(10).pdf" style="text-decoration:none;">Knowledge Distillation for Small-footprint Highway Networks </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(11).pdf" style="text-decoration:none;">Deep Model Compression: Distilling Knowledge from Noisy Teachers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(12).pdf" style="text-decoration:none;">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(13).pdf" style="text-decoration:none;">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(14).pdf" style="text-decoration:none;">Deep Mutual Learning</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(15).pdf" style="text-decoration:none;">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(16).pdf" style="text-decoration:none;">DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(17).pdf" style="text-decoration:none;">Rocket Launching: A Universal and Efficient Framework for Training Well-performing Light Net</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(18).pdf" style="text-decoration:none;">Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(19).pdf" style="text-decoration:none;">Knowledge Projection for Effective Design of Thinner and Faster Deep Neural Networks</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(20).pdf" style="text-decoration:none;">Moonshine: Distilling with Cheap Convolutions</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(21).pdf" style="text-decoration:none;">Data Distillation: Towards Omni-Supervised Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(22).pdf" style="text-decoration:none;">Learning Global Additive Explanations for Neural Nets Using Model Distillation</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(23).pdf" style="text-decoration:none;">Large scale distributed neural network training through online distillation</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(24).pdf" style="text-decoration:none;">Training a Binary Weight Object Detector by Knowledge Transfer for Autonomous Driving</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(25).pdf" style="text-decoration:none;">Quantization Mimic: Towards Very Tiny CNN for Object Detection</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(26).pdf" style="text-decoration:none;">Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(27).pdf" style="text-decoration:none;">YASENN: Explaining Neural Networks via Partitioning Activation Sequences</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(28).pdf" style="text-decoration:none;">Dataset Distillation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(29).pdf" style="text-decoration:none;">Learning Efficient Detector with Semi-supervised Adaptive Distillation </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(30).pdf" style="text-decoration:none;">Improved Knowledge Distillation via Teacher Assistant</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(31).pdf" style="text-decoration:none;">ResKD: Residual-Guided Knowledge Distillation</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(32).pdf" style="text-decoration:none;">Model Compression</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(33).pdf" style="text-decoration:none;">Combining Labeled and Unlabeled Data with Co-Training</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(34).pdf" style="text-decoration:none;">Cross Modal Distillation for Supervision Transfer</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(35).pdf" style="text-decoration:none;">Local Affine Approximators for Improving Knowledge Transfer</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(36).pdf" style="text-decoration:none;">Knowledge Acquisition from Examples Via Multiple Models</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(37).pdf" style="text-decoration:none;">Learning Efficient Object Detection Models with Knowledge Distillation</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(38).pdf" style="text-decoration:none;">A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Knowledge-Distillation-Papers/blob/master/knd(39).pdf" style="text-decoration:none;">Using a Neural Network to Approximate an Ensemble of Classifiers</a></li>
 </ul>
  
  
